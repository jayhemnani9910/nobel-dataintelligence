{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f482a8bf",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "985282bf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:47.032121Z",
     "iopub.status.busy": "2025-12-17T01:21:47.031799Z",
     "iopub.status.idle": "2025-12-17T01:21:48.729665Z",
     "shell.execute_reply": "2025-12-17T01:21:48.729112Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from collections import Counter\n",
    "\n",
    "# Ensure repo root is on sys.path (works from repo root or notebooks/)\n",
    "repo_root = Path.cwd()\n",
    "if not (repo_root / 'src').exists():\n",
    "    repo_root = repo_root.parent\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_style('whitegrid')\n",
    "except ImportError:\n",
    "    sns = None\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from src.data_acquisition import KaggleDataAcquisition\n",
    "from src.datasets import CAFA5Dataset\n",
    "from src.models.multimodal import VibroStructuralModel\n",
    "from src.models.losses import FocalLoss, WeightedBCELoss\n",
    "from src.training import Trainer, MetricComputer\n",
    "from src.utils import Logger, set_seed, get_device, batch_collate_function\n",
    "\n",
    "logger = Logger.setup('QDD-CAFA5', level=logging.INFO)\n",
    "set_seed(42)\n",
    "device = get_device()\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 6)\n",
    "\n",
    "data_cafa_dir = repo_root / 'data' / 'cafa5'\n",
    "spectral_dir = data_cafa_dir / 'spectral'\n",
    "structures_dir = data_cafa_dir / 'structures'\n",
    "checkpoints_dir = repo_root / 'checkpoints'\n",
    "for p in [data_cafa_dir, spectral_dir, structures_dir, checkpoints_dir]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "logger.info('Setup complete!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c509504",
   "metadata": {},
   "source": [
    "## 2. Download and Explore CAFA 5 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b40e5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:48.731016Z",
     "iopub.status.busy": "2025-12-17T01:21:48.730802Z",
     "iopub.status.idle": "2025-12-17T01:21:48.734826Z",
     "shell.execute_reply": "2025-12-17T01:21:48.734372Z"
    }
   },
   "outputs": [],
   "source": [
    "# Download CAFA 5 data\n",
    "logger.info(\"Downloading CAFA 5 competition data...\")\n",
    "kaggle_acq = KaggleDataAcquisition(output_dir=str(data_cafa_dir))\n",
    "\n",
    "# Note: Uncomment to download\n",
    "# train_terms, test_seqs, go_annot = kaggle_acq.download_cafa5()\n",
    "\n",
    "# For this demo, check if files exist\n",
    "train_terms_file = data_cafa_dir / 'train_terms.csv'\n",
    "train_seqs_file = data_cafa_dir / 'train_sequences.fasta'\n",
    "test_seqs_file = data_cafa_dir / 'test_sequences.fasta'\n",
    "go_vocab_file = data_cafa_dir / 'go_vocabulary.csv'\n",
    "\n",
    "if not train_terms_file.exists():\n",
    "    logger.warning(\"CAFA 5 data not found. Please run: kaggle competitions download -c cafa-5-protein-function-prediction -p ./data/cafa5\")\n",
    "else:\n",
    "    logger.info(\"CAFA 5 data found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfed7bc",
   "metadata": {},
   "source": [
    "## 3. Load and Explore GO Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea084d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:48.735932Z",
     "iopub.status.busy": "2025-12-17T01:21:48.735824Z",
     "iopub.status.idle": "2025-12-17T01:21:48.747371Z",
     "shell.execute_reply": "2025-12-17T01:21:48.746976Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load training terms (normalize columns to protein_id/go_term for this notebook)\n",
    "try:\n",
    "    df_terms = pd.read_csv(train_terms_file)\n",
    "    # Normalize common column names\n",
    "    if {'target_id', 'go_id'}.issubset(df_terms.columns):\n",
    "        df_terms = df_terms.rename(columns={'target_id': 'protein_id', 'go_id': 'go_term'})\n",
    "    elif {'EntryID', 'term'}.issubset(df_terms.columns):\n",
    "        df_terms = df_terms.rename(columns={'EntryID': 'protein_id', 'term': 'go_term'})\n",
    "    elif {'entry_id', 'term'}.issubset(df_terms.columns):\n",
    "        df_terms = df_terms.rename(columns={'entry_id': 'protein_id', 'term': 'go_term'})\n",
    "\n",
    "    logger.info(f\"Training terms shape: {df_terms.shape}\")\n",
    "    logger.info(f\"\\nFirst few rows:\")\n",
    "    print(df_terms.head(10))\n",
    "\n",
    "    logger.info(f\"\\nData info:\")\n",
    "    print(df_terms.info())\n",
    "except FileNotFoundError:\n",
    "    logger.warning(\"train_terms.csv not found. Creating a demo dataset so the notebook can run end-to-end.\")\n",
    "    df_terms = pd.DataFrame({\n",
    "        'protein_id': [f'protein_{i}' for i in range(500)],\n",
    "        'go_term': [f'GO:{np.random.randint(1000000, 9999999):07d}' for _ in range(500)]\n",
    "    })\n",
    "    df_terms.to_csv(train_terms_file, index=False)\n",
    "    logger.info(f\"Demo dataset created: {df_terms.shape[0]} annotations\")\n",
    "\n",
    "# Ensure we have sequence FASTA files for training/inference\n",
    "if not train_seqs_file.exists():\n",
    "    proteins = sorted(df_terms['protein_id'].unique())[:200]\n",
    "    with open(train_seqs_file, 'w') as f:\n",
    "        for pid in proteins:\n",
    "            f.write(f\">{pid}\\nACDEFGHIKLMNPQRSTVWY\\n\")\n",
    "    logger.info(f\"Wrote demo train_sequences.fasta with {len(proteins)} proteins\")\n",
    "\n",
    "if not test_seqs_file.exists():\n",
    "    with open(test_seqs_file, 'w') as f:\n",
    "        for i in range(50):\n",
    "            f.write(f\">test_{i}\\nACDEFGHIKLMNPQRSTVWY\\n\")\n",
    "    logger.info(\"Wrote demo test_sequences.fasta with 50 proteins\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84de4a8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:48.748764Z",
     "iopub.status.busy": "2025-12-17T01:21:48.748655Z",
     "iopub.status.idle": "2025-12-17T01:21:48.759604Z",
     "shell.execute_reply": "2025-12-17T01:21:48.759132Z"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze GO term distribution\n",
    "logger.info(\"Analyzing GO term distribution...\")\n",
    "\n",
    "# Count unique proteins and terms\n",
    "n_proteins = df_terms['protein_id'].nunique()\n",
    "n_go_terms = df_terms['go_term'].nunique()\n",
    "n_annotations = len(df_terms)\n",
    "\n",
    "logger.info(f\"Unique proteins: {n_proteins:,}\")\n",
    "logger.info(f\"Unique GO terms: {n_go_terms:,}\")\n",
    "logger.info(f\"Total annotations: {n_annotations:,}\")\n",
    "logger.info(f\"Avg terms per protein: {n_annotations / n_proteins:.2f}\")\n",
    "\n",
    "# Analyze term frequency\n",
    "term_counts = df_terms['go_term'].value_counts()\n",
    "logger.info(f\"\\nGO term frequency:\")\n",
    "logger.info(f\"  Max: {term_counts.max()}\")\n",
    "logger.info(f\"  Min: {term_counts.min()}\")\n",
    "logger.info(f\"  Median: {term_counts.median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be23c0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:48.760712Z",
     "iopub.status.busy": "2025-12-17T01:21:48.760598Z",
     "iopub.status.idle": "2025-12-17T01:21:48.943950Z",
     "shell.execute_reply": "2025-12-17T01:21:48.943542Z"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize GO distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Top GO terms\n",
    "top_go = df_terms['go_term'].value_counts().head(15)\n",
    "axes[0].barh(range(len(top_go)), top_go.values, color='steelblue')\n",
    "axes[0].set_yticks(range(len(top_go)))\n",
    "axes[0].set_yticklabels(top_go.index, fontsize=9)\n",
    "axes[0].set_xlabel('Frequency', fontsize=11)\n",
    "axes[0].set_title('Top 15 Most Frequent GO Terms', fontsize=12, fontweight='bold')\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Terms per protein distribution\n",
    "terms_per_protein = df_terms.groupby('protein_id').size()\n",
    "axes[1].hist(terms_per_protein, bins=30, color='coral', edgecolor='black', alpha=0.7)\n",
    "axes[1].set_xlabel('Number of GO Terms', fontsize=11)\n",
    "axes[1].set_ylabel('Number of Proteins', fontsize=11)\n",
    "axes[1].set_title('GO Terms per Protein Distribution', fontsize=12, fontweight='bold')\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "logger.info(\"GO analysis complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab2ab29",
   "metadata": {},
   "source": [
    "## 4. Create GO Term Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fdcf33",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:48.945481Z",
     "iopub.status.busy": "2025-12-17T01:21:48.945337Z",
     "iopub.status.idle": "2025-12-17T01:21:48.956087Z",
     "shell.execute_reply": "2025-12-17T01:21:48.955466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build GO vocabulary\n",
    "logger.info(\"Building GO term vocabulary...\")\n",
    "\n",
    "unique_go_terms = sorted(df_terms['go_term'].unique())\n",
    "go_to_idx = {go: idx for idx, go in enumerate(unique_go_terms)}\n",
    "idx_to_go = {idx: go for go, idx in go_to_idx.items()}\n",
    "\n",
    "logger.info(f\"Vocabulary size: {len(go_to_idx)} GO terms\")\n",
    "logger.info(\"\\nSample terms:\")\n",
    "for go_term in unique_go_terms[:10]:\n",
    "    idx = go_to_idx[go_term]\n",
    "    count = (df_terms['go_term'] == go_term).sum()\n",
    "    logger.info(f\"  {idx}: {go_term} (n={count})\")\n",
    "\n",
    "# Save vocabulary (use repo_root-relative paths; safe when executed from notebooks/)\n",
    "vocab_df = pd.DataFrame({\n",
    "    'go_term': unique_go_terms,\n",
    "    'index': [go_to_idx[go] for go in unique_go_terms]\n",
    "})\n",
    "vocab_df.to_csv(go_vocab_file, index=False)\n",
    "logger.info(f\"\\nVocabulary saved to {go_vocab_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d29253",
   "metadata": {},
   "source": [
    "## 5. Create Dataset and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5799e0e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:48.957295Z",
     "iopub.status.busy": "2025-12-17T01:21:48.957176Z",
     "iopub.status.idle": "2025-12-17T01:21:48.967603Z",
     "shell.execute_reply": "2025-12-17T01:21:48.967154Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create CAFA 5 dataset\n",
    "logger.info(\"Creating CAFA 5 dataset...\")\n",
    "\n",
    "dataset = CAFA5Dataset(\n",
    "    sequences_fasta=str(train_seqs_file),\n",
    "    terms_csv=str(train_terms_file),\n",
    "    spectra_dir=str(spectral_dir),\n",
    "    structure_dir=str(structures_dir),\n",
    "    go_terms_list=unique_go_terms,\n",
    ")\n",
    "logger.info(f\"Dataset created: {len(dataset)} proteins\")\n",
    "\n",
    "# Split into train/val and reuse val as test (robust for tiny demo datasets)\n",
    "n_total = len(dataset)\n",
    "if n_total <= 0:\n",
    "    raise ValueError(f\"No sequences found in {train_seqs_file}\")\n",
    "\n",
    "if n_total == 1:\n",
    "    train_dataset = dataset\n",
    "    val_dataset = dataset\n",
    "    test_dataset = dataset\n",
    "    train_size = val_size = test_size = 1\n",
    "elif n_total == 2:\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [1, 1])\n",
    "    test_dataset = val_dataset\n",
    "    train_size = val_size = test_size = 1\n",
    "else:\n",
    "    train_size = max(1, int(0.8 * n_total))\n",
    "    val_size = n_total - train_size\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "    test_dataset = val_dataset\n",
    "    test_size = val_size\n",
    "\n",
    "logger.info(f\"Split: train={train_size}, val={val_size}, test={test_size}\")\n",
    "\n",
    "# Create DataLoaders (important: collate_fn for PyG graphs)\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    collate_fn=batch_collate_function,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=batch_collate_function,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=batch_collate_function,\n",
    ")\n",
    "\n",
    "logger.info(\"DataLoaders created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff60b047",
   "metadata": {},
   "source": [
    "## 6. Initialize Model and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5a8a38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:48.968955Z",
     "iopub.status.busy": "2025-12-17T01:21:48.968836Z",
     "iopub.status.idle": "2025-12-17T01:21:50.572938Z",
     "shell.execute_reply": "2025-12-17T01:21:50.572449Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "logger.info(\"Initializing Vibro-Structural model for multi-label classification...\")\n",
    "\n",
    "num_go_terms = len(unique_go_terms)\n",
    "\n",
    "model = VibroStructuralModel(\n",
    "    latent_dim=128,\n",
    "    gnn_input_dim=24,\n",
    "    fusion_type='bilinear',\n",
    "    dropout=0.2,\n",
    "    num_go_terms=num_go_terms,\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.info(f\"Model: {total_params:,} total parameters, {trainable_params:,} trainable\")\n",
    "logger.info(f\"Output dimension: {num_go_terms} GO terms\")\n",
    "\n",
    "# Setup training\n",
    "optimizer = Adam(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "try:\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)\n",
    "except TypeError:\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Up-weight positive examples (simple baseline; tune on real data)\n",
    "pos_weight = torch.full((num_go_terms,), 2.0, dtype=torch.float32, device=device)\n",
    "loss_fn = WeightedBCELoss(pos_weight=pos_weight)\n",
    "# loss_fn = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler,\n",
    "    device=device,\n",
    "    checkpoint_dir=str(checkpoints_dir),\n",
    ")\n",
    "\n",
    "logger.info(\"Training setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7730ec",
   "metadata": {},
   "source": [
    "## 7. Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1552f00d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:50.574235Z",
     "iopub.status.busy": "2025-12-17T01:21:50.574010Z",
     "iopub.status.idle": "2025-12-17T01:21:53.571619Z",
     "shell.execute_reply": "2025-12-17T01:21:53.571064Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "logger.info(\"Starting training...\")\n",
    "logger.info(f\"Task: Multi-label GO term prediction ({num_go_terms} terms)\")\n",
    "logger.info(\"Metric: F-max score (mean F1 optimized over thresholds)\")\n",
    "\n",
    "best_loss = trainer.fit(\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    epochs=5,\n",
    "    metric_fn=MetricComputer.f_max,\n",
    "    early_stopping_patience=3,\n",
    "    task='cafa5',\n",
    ")\n",
    "\n",
    "logger.info(f\"\\nTraining complete! Best validation loss: {best_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5985a4",
   "metadata": {},
   "source": [
    "## 8. Evaluate with F-max Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "695e0022",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:53.573054Z",
     "iopub.status.busy": "2025-12-17T01:21:53.572928Z",
     "iopub.status.idle": "2025-12-17T01:21:53.595317Z",
     "shell.execute_reply": "2025-12-17T01:21:53.594915Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Evaluating on test split and selecting a threshold...\")\n",
    "\n",
    "model.eval()\n",
    "all_probs = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        graph = batch['graph'].to(device)\n",
    "        spectra = batch['spectra'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        global_features = batch.get('global_features')\n",
    "        if global_features is not None:\n",
    "            global_features = global_features.to(device)\n",
    "\n",
    "        logits = model(graph, spectra, global_features=global_features, task='cafa5')\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        all_probs.append(probs.cpu().numpy())\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "all_probs = np.concatenate(all_probs, axis=0)\n",
    "all_labels = np.concatenate(all_labels, axis=0)\n",
    "\n",
    "thresholds = np.arange(0.1, 0.95, 0.05)\n",
    "f_scores = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    preds_binary = (all_probs >= threshold).astype(int)\n",
    "\n",
    "    tp = np.sum((preds_binary == 1) & (all_labels == 1), axis=1)\n",
    "    fp = np.sum((preds_binary == 1) & (all_labels == 0), axis=1)\n",
    "    fn = np.sum((preds_binary == 0) & (all_labels == 1), axis=1)\n",
    "\n",
    "    precision = tp / (tp + fp + 1e-8)\n",
    "    recall = tp / (tp + fn + 1e-8)\n",
    "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
    "\n",
    "    f_scores.append(float(np.mean(f1)))\n",
    "\n",
    "best_threshold_idx = int(np.argmax(f_scores))\n",
    "best_threshold = float(thresholds[best_threshold_idx])\n",
    "best_f_max = float(f_scores[best_threshold_idx])\n",
    "\n",
    "logger.info(f\"Best F-max (mean F1): {best_f_max:.4f} at threshold {best_threshold:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3256a468",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:53.596800Z",
     "iopub.status.busy": "2025-12-17T01:21:53.596669Z",
     "iopub.status.idle": "2025-12-17T01:21:53.729815Z",
     "shell.execute_reply": "2025-12-17T01:21:53.729233Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot threshold sensitivity\n",
    "if dataset is not None:\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    ax.plot(thresholds, f_scores, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "    ax.axvline(best_threshold, color='red', linestyle='--', linewidth=2, label=f'Best: {best_threshold:.2f}')\n",
    "    ax.set_xlabel('Classification Threshold', fontsize=12)\n",
    "    ax.set_ylabel('F-max Score', fontsize=12)\n",
    "    ax.set_title('F-max Score vs Classification Threshold', fontsize=13, fontweight='bold')\n",
    "    ax.legend(fontsize=11)\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    logger.info(f\"Threshold sensitivity plot shown.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7a6d76",
   "metadata": {},
   "source": [
    "## 9. Generate Competition Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390928b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:53.731771Z",
     "iopub.status.busy": "2025-12-17T01:21:53.731620Z",
     "iopub.status.idle": "2025-12-17T01:21:53.772931Z",
     "shell.execute_reply": "2025-12-17T01:21:53.772338Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"Generating test-set predictions for submission...\")\n",
    "\n",
    "infer_dataset = CAFA5Dataset(\n",
    "    sequences_fasta=str(test_seqs_file),\n",
    "    terms_csv=None,\n",
    "    spectra_dir=str(spectral_dir),\n",
    "    structure_dir=str(structures_dir),\n",
    "    go_terms_list=unique_go_terms,\n",
    ")\n",
    "protein_ids = list(infer_dataset.sequences.keys())\n",
    "\n",
    "infer_loader = DataLoader(\n",
    "    infer_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    collate_fn=batch_collate_function,\n",
    ")\n",
    "\n",
    "threshold = best_threshold if 'best_threshold' in globals() else 0.5\n",
    "\n",
    "model.eval()\n",
    "rows = []\n",
    "idx_offset = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in infer_loader:\n",
    "        graph = batch['graph'].to(device)\n",
    "        spectra = batch['spectra'].to(device)\n",
    "\n",
    "        global_features = batch.get('global_features')\n",
    "        if global_features is not None:\n",
    "            global_features = global_features.to(device)\n",
    "\n",
    "        logits = model(graph, spectra, global_features=global_features, task='cafa5')\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        for p in probs:\n",
    "            pid = protein_ids[idx_offset]\n",
    "            idx_offset += 1\n",
    "\n",
    "            pred_idx = np.where(p >= threshold)[0]\n",
    "            terms = [idx_to_go[i] for i in pred_idx]\n",
    "            if not terms:\n",
    "                terms = ['GO:0005575']\n",
    "\n",
    "            rows.append({'protein_id': pid, 'go_terms': ' '.join(terms)})\n",
    "\n",
    "out_path = data_cafa_dir / 'submission.csv'\n",
    "df_submission = pd.DataFrame(rows)\n",
    "df_submission.to_csv(out_path, index=False)\n",
    "\n",
    "logger.info(f\"Submission file created: {out_path} ({len(df_submission)} proteins)\")\n",
    "print(df_submission.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec37ed6",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39b950",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-17T01:21:53.774314Z",
     "iopub.status.busy": "2025-12-17T01:21:53.774164Z",
     "iopub.status.idle": "2025-12-17T01:21:53.795626Z",
     "shell.execute_reply": "2025-12-17T01:21:53.795047Z"
    }
   },
   "outputs": [],
   "source": [
    "logger.info(\"\\n\" + \"=\"*60)\n",
    "logger.info(\"CAFA 5 Competition Execution Summary\")\n",
    "logger.info(\"=\"*60)\n",
    "logger.info(f\"Competition: CAFA 5 - Protein Function Prediction\")\n",
    "logger.info(f\"Task: Multi-label GO term prediction\")\n",
    "logger.info(f\"Metric: F-max score (optimized F1 across thresholds)\")\n",
    "logger.info(f\"Approach: Vibro-structural multimodal model\")\n",
    "logger.info(f\"\\nModel: VibroStructuralModel (Multi-label Head)\")\n",
    "logger.info(f\"  - GNN branch: Structural graph encoding\")\n",
    "logger.info(f\"  - CNN branch: Spectral fingerprint encoding\")\n",
    "logger.info(f\"  - Fusion: Bilinear transformation\")\n",
    "logger.info(f\"  - Head: Multi-label logistic regression ({num_go_terms} GO terms)\")\n",
    "logger.info(f\"\\nTraining Details:\")\n",
    "logger.info(f\"  - Loss: Weighted BCE (focal loss alternative available)\")\n",
    "logger.info(f\"  - Optimizer: Adam (lr=5e-4)\")\n",
    "logger.info(f\"  - Schedule: ReduceLROnPlateau (patience=5)\")\n",
    "logger.info(f\"  - Early stopping: patience=15\")\n",
    "logger.info(f\"\\nNext steps:\")\n",
    "logger.info(f\"  1. Download full CAFA 5 dataset\")\n",
    "logger.info(f\"  2. Retrieve 3D structures from AlphaFold DB\")\n",
    "logger.info(f\"  3. Precompute spectral features for all proteins\")\n",
    "logger.info(f\"  4. Implement hierarchical GO prediction (respecting ontology)\")\n",
    "logger.info(f\"  5. Train on full dataset (~30,000 proteins)\")\n",
    "logger.info(f\"  6. Optimize threshold for F-max metric\")\n",
    "logger.info(f\"  7. Ensemble with ESM-2 sequence embeddings\")\n",
    "logger.info(f\"  8. Generate final submission\")\n",
    "logger.info(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
